# GradFaith-CAM ğŸ”ğŸ”¥  
### *Seeing Isnâ€™t Always Believing: Faithfulness Analysis of Grad-CAM in Lung Cancer CT Classification*

<p align="center">
  <img src="results/figures/teaser.png" width="85%">
</p>

<p align="center">
  <b>KST 2026 Â· 18th International Conference on Knowledge and Smart Technology</b><br>
  Pattaya, Thailand Â· January 21â€“24, 2026
</p>

---

## ğŸ§  Motivation

Grad-CAM has become the *de facto* explainability tool for medical image analysis.  
But a critical question remains unanswered:

> **Do Grad-CAM heatmaps truly reflect the modelâ€™s reasoning â€” or are we just seeing convincing illusions?**

This repository accompanies our **KST-2026 accepted paper**, providing a **rigorous, quantitative evaluation of Grad-CAM faithfulness and localization reliability** across modern deep learning architectures for **lung cancer CT classification**.

---

## ğŸ“„ Paper

**Seeing Isnâ€™t Always Believing: Analysis of Grad-CAM Faithfulness and Localization Reliability in Lung Cancer CT Classification**  
ğŸ“ *KST 2026 (Accepted)*

**Authors:**  
- **Teerapong Panboonyuen** (Chulalongkorn University, MARSAIL)

---

## ğŸš€ Key Contributions

âœ… **Faithfulness-aware evaluation of Grad-CAM**  
âœ… **Cross-architecture analysis** (CNNs vs Vision Transformers)  
âœ… **Quantitative explanation metrics beyond visualization**  
âœ… **Exposure of shortcut learning and misleading saliency**  
âœ… **Clinical implications for trustworthy medical AI**

---

## ğŸ¥ Dataset

We evaluate on the publicly available **IQ-OTH/NCCD Lung Cancer CT Dataset**:

- **1,190 CT slices**
- **110 patients**
- Classes:
  - Normal
  - Benign
  - Malignant
- Expert annotations by radiologists & oncologists

> âš ï¸ All data are de-identified and ethically approved.

---

## ğŸ§© Models Evaluated

| Architecture | Type |
|--------------|------|
| ResNet-50 | CNN |
| ResNet-101 | CNN |
| DenseNet-161 | CNN |
| EfficientNet-B0 | CNN |
| ViT-Base-Patch16-224 | Transformer |

---

## ğŸ” What Is *GradFaith-CAM*?

We go beyond pretty heatmaps.

### âœ¨ Faithfulness Metrics Introduced

1ï¸âƒ£ **Localization Accuracy**
- Overlap between Grad-CAM maps and tumor regions

2ï¸âƒ£ **Perturbation-Based Faithfulness**
- Drop in confidence when highlighted regions are removed

3ï¸âƒ£ **Explanation Consistency**
- Stability across random seeds and model re-initializations

Together, these metrics answer a critical question:

> *Does the highlighted region actually matter for the prediction?*

---

## ğŸ“Š Key Findings

ğŸ”¥ **Grad-CAM is NOT uniformly reliable**

- CNNs often produce **coarse or misleading attention**
- DenseNet shows signs of **shortcut learning**
- ViT provides **precise but sometimes non-faithful localization**
- High accuracy â‰  trustworthy explanation

> **Seeing a heatmap does not mean believing the model.**

---

## ğŸ–¼ï¸ Visual Examples

<p align="center">
  <img src="results/figures/gradcam_comparison.png" width="100%">
</p>

---

## âš™ï¸ Installation

```bash
git clone https://github.com/yourusername/GradFaith-CAM.git
cd GradFaith-CAM
pip install -r requirements.txt
````

---

## ğŸ§ª Run Experiments

### Train a model

```bash
python experiments/train.py --config configs/resnet.yaml
```

### Evaluate Grad-CAM faithfulness

```bash
python experiments/evaluate.py --model resnet50
```

### Visualize explanations

```bash
python experiments/visualize.py --image sample.png
```

---

## ğŸ“Œ Why This Matters

Medical AI does not fail loudly â€” it fails *convincingly*.

This work shows why **blind trust in saliency maps is dangerous**, and why explainability must be:

* Quantitative
* Model-aware
* Clinically grounded

---

## ğŸ“š Citation

If you use this code, please cite:

```bibtex
@inproceedings{panboonyuen2026gradfaithcam,
  title     = {Seeing Isnâ€™t Always Believing: Analysis of Grad-CAM Faithfulness and Localization Reliability in Lung Cancer CT Classification},
  author    = {Panboonyuen, Teerapong},
  booktitle = {Proceedings of the 18th International Conference on Knowledge and Smart Technology (KST)},
  year      = {2026}
}
```

---

## ğŸ¤ Acknowledgements

This research was conducted at **Chulalongkorn University** and **MARSAIL (Motor AI Recognition Solution Artificial Intelligence Laboratory)**.

---

## ğŸ§  Final Thought

> **Interpretability without faithfulness is just another illusion.**

Letâ€™s build AI we can truly trust.

---